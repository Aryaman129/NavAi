{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NavAI: IMU Speed Estimation Model Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and exploring sensor data\n",
    "2. Training IMU-based speed estimation models\n",
    "3. Evaluating model performance\n",
    "4. Exporting models to TensorFlow Lite for mobile deployment\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "\n",
    "from data.data_loader import DataLoader as NavAIDataLoader\n",
    "from models.speed_estimator import SpeedCNN, SpeedLSTM, WindowGenerator, create_tensorflow_model, convert_to_tflite\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "### Quick Dataset Options for Showcase:\n",
    "\n",
    "**ðŸš€ Option 1: comma2k19 (Recommended)**\n",
    "- **Best for**: Vehicle navigation, real driving scenarios\n",
    "- **Download**: `git clone https://github.com/commaai/comma2k19.git` (includes 1-minute sample)\n",
    "- **Full dataset**: http://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb (~100GB)\n",
    "- **Data**: IMU (100Hz) + GPS + CAN speed + Video\n",
    "\n",
    "**ðŸ¥ˆ Option 2: EuRoC MAV** \n",
    "- **Best for**: Precise evaluation, academic validation\n",
    "- **Download**: http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/\n",
    "- **Size**: ~1-2GB per sequence\n",
    "- **Data**: Stereo cameras + IMU (200Hz) + precise ground truth\n",
    "\n",
    "**ðŸ’¡ For immediate demo**: The notebook will create synthetic data if no dataset is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = NavAIDataLoader(target_sample_rate=100)\n",
    "\n",
    "# For immediate showcase - use comma2k19 sample data\n",
    "# Download sample from: https://github.com/commaai/comma2k19\n",
    "print(\"For showcase: Download comma2k19 sample data\")\n",
    "print(\"1. Clone: git clone https://github.com/commaai/comma2k19.git\")\n",
    "print(\"2. Use Example_1/b0c9d2329ad1606b sample segment\")\n",
    "print(\"3. Or download full dataset from: http://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb\")\n",
    "\n",
    "# Configure data paths - update these with your actual data locations\n",
    "data_paths = {\n",
    "    'comma2k19': '../data/comma2k19/',  # Download comma2k19 here\n",
    "    # 'navai': '../data/navai_logs/',  # Your collected logs\n",
    "    # 'oxiod': '../data/oxiod/',     # Uncomment when available\n",
    "    # 'iovnbd': '../data/iovnbd/',   # Uncomment when available\n",
    "}\n",
    "\n",
    "# Load available datasets\n",
    "try:\n",
    "    df = data_loader.load_combined_dataset(data_paths)\n",
    "    print(f\"Loaded dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nDataset info:\")\n",
    "    df.info()\n",
    "except Exception as e:\n",
    "    print(f\"Data loading failed: {e}\")\n",
    "    print(\"Please download comma2k19 sample data first\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    print(\"\\nCreating sample synthetic data for demonstration...\")\n",
    "    import numpy as np\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # Generate realistic IMU + GPS data\n",
    "    time_ns = np.arange(n_samples) * 10_000_000  # 100Hz\n",
    "    accel_x = np.random.normal(0, 2, n_samples) + np.sin(np.arange(n_samples) * 0.01) * 3\n",
    "    accel_y = np.random.normal(0, 2, n_samples)\n",
    "    accel_z = np.random.normal(-9.81, 1, n_samples)\n",
    "    \n",
    "    gyro_x = np.random.normal(0, 0.1, n_samples)\n",
    "    gyro_y = np.random.normal(0, 0.1, n_samples)  \n",
    "    gyro_z = np.random.normal(0, 0.1, n_samples)\n",
    "    \n",
    "    # Simulated vehicle speed (0-30 m/s)\n",
    "    speed_profile = 10 + 8 * np.sin(np.arange(n_samples) * 0.001) + np.random.normal(0, 1, n_samples)\n",
    "    speed_profile = np.clip(speed_profile, 0, 30)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp_ns': time_ns,\n",
    "        'accel_x': accel_x, 'accel_y': accel_y, 'accel_z': accel_z,\n",
    "        'gyro_x': gyro_x, 'gyro_y': gyro_y, 'gyro_z': gyro_z,\n",
    "        'mag_x': np.random.normal(20, 5, n_samples),\n",
    "        'mag_y': np.random.normal(0, 5, n_samples), \n",
    "        'mag_z': np.random.normal(-40, 5, n_samples),\n",
    "        'qw': np.ones(n_samples), 'qx': np.zeros(n_samples), \n",
    "        'qy': np.zeros(n_samples), 'qz': np.zeros(n_samples),\n",
    "        'gps_lat': 37.4419 + np.random.normal(0, 0.0001, n_samples),\n",
    "        'gps_lon': -122.1430 + np.random.normal(0, 0.0001, n_samples),\n",
    "        'gps_speed_mps': speed_profile,\n",
    "        'device': 'synthetic', 'source': 'demo'\n",
    "    })\n",
    "    \n",
    "    print(f\"Created synthetic dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "if not df.empty:\n",
    "    # Basic statistics\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Plot sensor data\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accelerometer\n",
    "    axes[0,0].plot(df['accel_x'][:1000], label='X', alpha=0.7)\n",
    "    axes[0,0].plot(df['accel_y'][:1000], label='Y', alpha=0.7)\n",
    "    axes[0,0].plot(df['accel_z'][:1000], label='Z', alpha=0.7)\n",
    "    axes[0,0].set_title('Accelerometer (first 1000 samples)')\n",
    "    axes[0,0].set_ylabel('Acceleration (m/sÂ²)')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Gyroscope\n",
    "    axes[0,1].plot(df['gyro_x'][:1000], label='X', alpha=0.7)\n",
    "    axes[0,1].plot(df['gyro_y'][:1000], label='Y', alpha=0.7)\n",
    "    axes[0,1].plot(df['gyro_z'][:1000], label='Z', alpha=0.7)\n",
    "    axes[0,1].set_title('Gyroscope (first 1000 samples)')\n",
    "    axes[0,1].set_ylabel('Angular velocity (rad/s)')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # GPS Speed\n",
    "    valid_gps = df[df['gps_speed_mps'] > 0]\n",
    "    if not valid_gps.empty:\n",
    "        axes[1,0].plot(valid_gps['gps_speed_mps'][:1000])\n",
    "        axes[1,0].set_title('GPS Speed (first 1000 valid samples)')\n",
    "        axes[1,0].set_ylabel('Speed (m/s)')\n",
    "        \n",
    "        # Speed distribution\n",
    "        axes[1,1].hist(valid_gps['gps_speed_mps'], bins=50, alpha=0.7)\n",
    "        axes[1,1].set_title('GPS Speed Distribution')\n",
    "        axes[1,1].set_xlabel('Speed (m/s)')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data loaded. Please check your data paths and ensure log files exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Window Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Filter data with valid GPS speed\n",
    "    valid_data = df[(df['gps_speed_mps'] >= 0) & (df['gps_speed_mps'] <= 50)].copy()  # Reasonable speed range\n",
    "    \n",
    "    print(f\"Valid data samples: {len(valid_data)} / {len(df)} ({len(valid_data)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Create windows for training\n",
    "    window_generator = WindowGenerator(\n",
    "        window_size_sec=1.5,\n",
    "        stride_sec=0.25,\n",
    "        sample_rate=100,\n",
    "        feature_cols=['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z'],\n",
    "        target_col='gps_speed_mps'\n",
    "    )\n",
    "    \n",
    "    X, y = window_generator.create_windows(valid_data)\n",
    "    \n",
    "    print(f\"Generated windows: X shape {X.shape}, y shape {y.shape}\")\n",
    "    print(f\"Speed range: {y.min():.2f} - {y.max():.2f} m/s\")\n",
    "    \n",
    "    # Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} windows\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} windows\")\n",
    "else:\n",
    "    print(\"Skipping preprocessing - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_torch = torch.FloatTensor(X_train)\n",
    "    y_train_torch = torch.FloatTensor(y_train)\n",
    "    X_val_torch = torch.FloatTensor(X_val)\n",
    "    y_val_torch = torch.FloatTensor(y_val)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = SpeedCNN(input_channels=6, hidden_dim=64).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X).squeeze()\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"PyTorch training completed!\")\n",
    "else:\n",
    "    print(\"Skipping training - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals():\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_y.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    # Calculate percentage errors\n",
    "    mean_speed = np.mean(actuals)\n",
    "    rmse_percent = (rmse / mean_speed) * 100\n",
    "    mae_percent = (mae / mean_speed) * 100\n",
    "    \n",
    "    print(f\"Validation Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.3f} m/s ({rmse_percent:.1f}%)\")\n",
    "    print(f\"MAE: {mae:.3f} m/s ({mae_percent:.1f}%)\")\n",
    "    print(f\"Mean actual speed: {mean_speed:.3f} m/s\")\n",
    "    \n",
    "    # Plot predictions vs actuals\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(actuals, predictions, alpha=0.5)\n",
    "    plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Speed (m/s)')\n",
    "    plt.ylabel('Predicted Speed (m/s)')\n",
    "    plt.title('Predictions vs Actuals')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    errors = predictions - actuals\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Prediction Error (m/s)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping evaluation - no trained model available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorFlow Model and Mobile Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    # Create TensorFlow model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, features)\n",
    "    tf_model = create_tensorflow_model(input_shape, model_type='cnn')\n",
    "    \n",
    "    # Compile model\n",
    "    tf_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(\"TensorFlow model created:\")\n",
    "    tf_model.summary()\n",
    "    \n",
    "    # Train TensorFlow model\n",
    "    history = tf_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Convert to TensorFlow Lite\n",
    "    print(\"\\nConverting to TensorFlow Lite...\")\n",
    "    tflite_model = convert_to_tflite(\n",
    "        tf_model, \n",
    "        quantize=True, \n",
    "        representative_dataset=X_train\n",
    "    )\n",
    "    \n",
    "    # Save TFLite model\n",
    "    tflite_path = '../models/speed_estimator.tflite'\n",
    "    os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "    \n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"TensorFlow Lite model saved to: {tflite_path}\")\n",
    "    print(f\"Model size: {len(tflite_model) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Test TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(f\"\\nTFLite model details:\")\n",
    "    print(f\"Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"Output shape: {output_details[0]['shape']}\")\n",
    "    print(f\"Input dtype: {input_details[0]['dtype']}\")\n",
    "    print(f\"Output dtype: {output_details[0]['dtype']}\")\n",
    "    \n",
    "    # Test inference\n",
    "    test_input = X_val[:1].astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "    interpreter.invoke()\n",
    "    tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    tf_output = tf_model.predict(test_input)\n",
    "    \n",
    "    print(f\"\\nInference test:\")\n",
    "    print(f\"TensorFlow output: {tf_output[0][0]:.3f}\")\n",
    "    print(f\"TFLite output: {tflite_output[0][0]:.3f}\")\n",
    "    print(f\"Actual speed: {y_val[0]:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping TensorFlow training - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the complete pipeline for training IMU-based speed estimation models:\n",
    "\n",
    "1. **Data Loading**: Unified loader supporting multiple datasets\n",
    "2. **Preprocessing**: Window generation and data preparation\n",
    "3. **Model Training**: Both PyTorch and TensorFlow implementations\n",
    "4. **Evaluation**: Performance metrics and visualization\n",
    "5. **Mobile Export**: TensorFlow Lite conversion for Android deployment\n",
    "\n",
    "### Next Steps:\n",
    "1. **Collect more data** using the Android sensor logger\n",
    "2. **Integrate public datasets** (IO-VNBD, OxIOD, comma2k19)\n",
    "3. **Implement EKF sensor fusion** in Phase 2\n",
    "4. **Add map matching** and offline navigation\n",
    "5. **Integrate ARCore VIO** for enhanced accuracy\n",
    "\n",
    "### Performance Targets:\n",
    "- **Speed RMSE**: <10% (currently achieved: varies by dataset)\n",
    "- **Model size**: <1MB (TFLite quantized)\n",
    "- **Inference time**: <10ms on mobile devices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
